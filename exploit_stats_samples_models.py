
# Quelques imports

import torch
import torch.nn as nn
import torchvision
from models.cnn import mnistConv
from matplotlib import pyplot as plt
import torch.nn.functional as F
import numpy as np
from scipy.special import softmax

from models.explainer import ExplainerMnist
from torch.optim import Adam

import pickle as pk
from scipy.stats import wasserstein_distance

stats = pk.load(open('stats/stats_linf_fine.dat', 'rb'))


class GeneraotrFromProbaMaps(nn.Module):
    def __init__(self, proba_maps, device='cpu'):
        super().__init__()

        self.device = device
        self.proba_maps = proba_maps.to(self.device)

    def reparametrize(self, proba, k, temperature=0.1):
        img = 0
        for _ in range(k):
            gumbel_noise = - torch.log(-torch.log(torch.rand_like(proba))).to(self.device)

            x = (torch.log(proba) + gumbel_noise) / temperature

            sample = nn.functional.softmax(x, dim=-1)
            # img.append(sample)
            img += sample

        # img = torch.stack(img, dim=-1)
        # img, indices = torch.max(img, dim=-1)

        return img

    def forward(self, y, k):
        proba = self.proba_maps[y]
        return self.reparametrize(proba, k=k), proba


def sliced_wasserstein(X, Y, num_proj):
    X = X.reshape((28,28))
    Y = Y.reshape((28,28))
    dim = 28
    ests = []
    for _ in range(num_proj):
        # sample uniformly from the unit sphere
        dir = np.random.rand(dim)
        dir /= np.linalg.norm(dir)

        # project the data
        X_proj = X @ dir
        Y_proj = Y @ dir

        # compute 1d wasserstein
        ests.append(wasserstein_distance(X_proj, Y_proj))
    return np.mean(ests)


def BC(p, q):
    s = 0
    p = torch.Tensor(p)
    q = torch.Tensor(q)

    return np.sqrt(1-torch.sqrt(p*q).sum().detach().cpu().numpy())


def BC_matrix(maplist):
    n = len(maplist)

    matrix = []
    for i in range(n):
        for j in range(n):
            if i == j:
                matrix.append(1)
            else:
                matrix.append(
                    BC(softmax(maplist[i].reshape(28 * 28), axis=0), softmax(maplist[j].reshape(28 * 28), axis=0)))

    return matrix

def wasserstein_matrix(maplist):
    n = len(maplist)

    matrix = []
    for i in range(n):
        for j in range(n):
            if i == j:
                matrix.append(1)
            else:
                matrix.append(
                    sliced_wasserstein(softmax(maplist[i].reshape(28 * 28), axis=0), softmax(maplist[j].reshape(28 * 28), axis=0), num_proj=10))

    return matrix

def xlogx(x):
    mask = (x > 1E-8).float()
    return (x * torch.log(x + 1E-7)) * mask


def entropy(X):
    X = torch.tensor(X)
    return - xlogx(X).sum().detach().cpu().numpy()

def entropy_list(maplist):
    n = len(maplist)

    matrix = []
    for i in range(n):
        matrix.append(entropy(softmax(maplist[i].reshape(28 * 28), axis=0)))

    return matrix




N = len(stats)
# adv_trains = [0.1, 0.5, 1., 1.5, 2., 5., 10., 20., 30., 50.]
adv_trains = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1., 1.2, 1.4, 1.6]
# adv_trains = [0.1, 1., 5., 10., 15., 20., 25., 30., 35., 40., 45., 50., 55., 60., 70., 80., 100.]

#attack_names = ["L2PGDAttack", "L1PGDAttack"]
attack_names = ["LinfPGDAttack"]
names = [f"{attack_name}_{str(adv)}" for adv in adv_trains for attack_name in attack_names ]

# for t in range(10):
#
#     for attack_name in attack_names:
#         for i in range(10):
#             fig, axs = plt.subplots(10, len(adv_trains), figsize=(10, 10))
#
#             for k, key in enumerate(adv_trains):
#                 v = stats[f"{attack_name}_{str(key)}"]
#
#                 maps = torch.Tensor(v['proba_maps'][t])
#
#                 gen = GeneraotrFromProbaMaps(maps, device='cuda')
#
#                 samples = gen.forward(torch.Tensor([i for _ in range(10)]).long(), k=500)[0].detach().cpu().numpy()
#                 for p in range(10):
#                     axs[p, k].imshow(samples[i].reshape((28,28)), cmap="gray")
#                     if p == 0:
#                         axs[p][k].set_title(key)
#                     axs[p][k].axis('off')
#
#             fig.tight_layout()
#             fig.savefig(f"tmp/samples_{attack_name}_fine_{t}_{i}")


for t in range(1):

    for attack_name in attack_names:

            for k, key in enumerate(adv_trains):
                v = stats[f"{attack_name}_{str(key)}"]
                fig, axs = plt.subplots(10, 10, figsize=(10, 10))

                model = mnistConv(device='cuda').to('cuda')
                model.load_state_dict(v['models'][t])

                for i in range(10):
                    maps = torch.Tensor(v['proba_maps'][t])

                    gen = GeneraotrFromProbaMaps(maps, device='cuda')

                    samples = gen.forward(torch.Tensor([i for _ in range(5)]).long(), k=500)[0]

                    probas = torch.exp(model(samples.view((5,1,28,28))))

                    samples = samples.detach().cpu().numpy()
                    probas = probas.detach().cpu().numpy()
                    for p in range(5):
                        axs[2*p, i].imshow(samples[p].reshape((28,28)), cmap="gray")
                        if p == 0:
                            axs[2*p][i].set_title("Class: " + str(i))
                        axs[2*p][i].axis('off')

                        axs[2*p+1][i].bar([c for c in range(10)], probas[p])
                        xloc = [np.argmax(probas[p])]
                        axs[2 * p + 1][i].set_xticks(xloc)
                        axs[2 * p + 1][i].get_yaxis().set_visible(False)

                fig.tight_layout()
                fig.savefig(f"tmp/samples_{attack_name}_{str(key)}_fine_{t}_{i}.png")






